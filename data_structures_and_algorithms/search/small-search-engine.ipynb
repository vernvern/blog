{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小型搜索引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link(id=0, url='www.test.com/0')\n",
      "- 未爬取？--> False\n",
      "--------------------\n",
      "Link(id=1, url='www.test.com/1')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test1'}\n",
      "--------------------\n",
      "Link(id=2, url='www.test.com/2')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test2'}\n",
      "--------------------\n",
      "Link(id=3, url='www.test.com/3')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'test3', 't'}\n",
      "--------------------\n",
      "Link(id=4, url='www.test.com/4')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test4'}\n",
      "--------------------\n",
      "Link(id=5, url='www.test.com/5')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test5'}\n",
      "--------------------\n",
      "Link(id=6, url='www.test.com/6')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test6'}\n",
      "--------------------\n",
      "Link(id=7, url='www.test.com/7')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test7'}\n",
      "--------------------\n",
      "Link(id=8, url='www.test.com/8')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test8'}\n",
      "--------------------\n",
      "Link(id=9, url='www.test.com/9')\n",
      "- 未爬取？--> True\n",
      "- 过滤、分词 --> {'t', 'test9'}\n",
      "--------------------\n",
      "['www.test.com/1']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, bloom_filter=None, length=0):\n",
    "\n",
    "        self.BIT_MAX = 10\n",
    "        \n",
    "        if bloom_filter:\n",
    "            self._bloom_filter = bloom_filter\n",
    "            self.LENGTH = len(bloom_filter)\n",
    "        else:\n",
    "            self.LENGTH = length // self.BIT_MAX\n",
    "            self._bloom_filter = [0 for x in range(self.LENGTH)]\n",
    "            \n",
    "    def note_url(self, url):\n",
    "        self._set_hash_1_in_bloom_filter(url)\n",
    "        self._set_hash_2_in_bloom_filter(url)\n",
    "        \n",
    "    def is_visited(self, url):\n",
    "        result1 = self._get_hash_1_in_bloom_filter(url)\n",
    "        result2 = self._get_hash_2_in_bloom_filter(url)  \n",
    "        return result1 and result2\n",
    "\n",
    "    def _hash_1(self, url):\n",
    "        md5 = hashlib.md5()\n",
    "        md5.update('www.test.com/0'.encode('utf-8'))\n",
    "        return hash(md5.hexdigest()) % (self.BIT_MAX * self.LENGTH)\n",
    "\n",
    "    def _hash_2(self, url):\n",
    "        return hash(url) % (self.BIT_MAX * self.LENGTH)\n",
    "    \n",
    "    def _get_hash_1_in_bloom_filter(self, url):\n",
    "        hash_ = self._hash_1(url)\n",
    "        index = hash_ // self.BIT_MAX\n",
    "        bit = hash_ % self.BIT_MAX\n",
    "        return bool(self._bloom_filter[index] >> bit)\n",
    "    \n",
    "    def _get_hash_2_in_bloom_filter(self, url):\n",
    "        hash_ = self._hash_2(url)\n",
    "        index = hash_ // self.BIT_MAX\n",
    "        bit = hash_ % self.BIT_MAX\n",
    "        return bool(self._bloom_filter[index] >> bit)\n",
    "    \n",
    "    def _set_hash_1_in_bloom_filter(self, url):\n",
    "        hash_ = self._hash_1(url)\n",
    "        index = hash_ // self.BIT_MAX\n",
    "        bit = hash_ % self.BIT_MAX\n",
    "        self._bloom_filter[index] += 2**bit\n",
    "    \n",
    "    def _set_hash_2_in_bloom_filter(self, url):\n",
    "        hash_ = self._hash_2(url)\n",
    "        index = hash_  // self.BIT_MAX\n",
    "        bit = hash_ % self.BIT_MAX\n",
    "        self._bloom_filter[index] += 2**bit\n",
    "\n",
    "        \n",
    "@dataclass\n",
    "class Link:\n",
    "    ''' 属性是和url相关的一些数据，提供关于html一些操作 '''\n",
    "    id: str\n",
    "    url: str\n",
    "    body: str= field(repr=False, init=False)\n",
    "    \n",
    "    def get(self):\n",
    "        ''' 爬取网页\n",
    "        \n",
    "        实际代码应该在这里爬取网页并且保存html文本\n",
    "        '''\n",
    "        self.body = '<p>test' + str(self.id) + \"</p><p>t</p>\"\n",
    "        self.body += '<script>balabala</script>'\n",
    "\n",
    "    def filter_words(self):\n",
    "        ''' 过滤html，获取词列表 '''\n",
    "        body = self.body\n",
    "        body = Link._moved_scrpit(body)\n",
    "        body = Link._moved_tag(body)\n",
    "        result = Link._split(body)\n",
    "        return result\n",
    "    \n",
    "    def get_links(self):\n",
    "        ''' 提取跳转链接 '''\n",
    "        # TODO:  提取跳转链接\n",
    "        return []\n",
    "        \n",
    "    @staticmethod\n",
    "    def _moved_scrpit(body):\n",
    "        ''' 去掉js、css、下拉框等用户看不到的内容 '''\n",
    "        _body = re.sub(r'<script>.*</script>', '', body)\n",
    "        return _body\n",
    "    \n",
    "    @staticmethod\n",
    "    def _moved_tag(body):\n",
    "        ''' 去掉html标签 '''\n",
    "        _body = re.sub(r'</?\\w*>?', '\\t', body)\n",
    "        return _body\n",
    "    \n",
    "    @staticmethod\n",
    "    def _split(body):\n",
    "        ''' 分词 '''\n",
    "        return set([x for x in body.split('\\t') if x])\n",
    "\n",
    "@dataclass\n",
    "class Index:\n",
    "    word_id: int\n",
    "    urls: list = field(default_factory=list)\n",
    "    \n",
    "    def load(self, string):\n",
    "        word_id, urls = string.split('\\t')\n",
    "        urls = urls.split(',')\n",
    "        self.word_id = word_id\n",
    "        self.urls = urls\n",
    "    \n",
    "    def to_inverted_index(self):\n",
    "        return '%s\\t%s' % (self.word_id, ','.join(self.urls))\n",
    "\n",
    "    \n",
    "# 这四个变量可以存到文件里\n",
    "links = ['www.test.com/' + str(x) for x in range(10)]  # 待爬取的网页url\n",
    "bloom_filter = 0  # 用来判重的bloom过滤器\n",
    "doc_raw = ''  # 保存已爬取的网页数据，实际上不应该放在内存，由于比较大应该保存到文件\n",
    "doc_ids = {}  # 网页id: 网页url\n",
    "\n",
    "BLOOM_LENGTH = 10240  # 布隆过滤器位数\n",
    "_url_id = 0  # url id 计数器\n",
    "def new_url_id():\n",
    "    global _url_id\n",
    "    _id = _url_id\n",
    "    _url_id += 1\n",
    "    return _id\n",
    "\n",
    "\n",
    "# 初始化布隆过滤器\n",
    "bloom = BloomFilter(length=BLOOM_LENGTH)    \n",
    "bloom.note_url('www.test.com/0')\n",
    "\n",
    "# 倒排索引\n",
    "inverted_index = ['0\\twww.1.com,www.2.com']\n",
    "# 词和id对应关系\n",
    "word_ids = {}\n",
    "# 词id和在倒排索引中的位置对应关系\n",
    "word_id_position = {1: 0}\n",
    "# 词id\n",
    "_word_id = 1\n",
    "def new_word_id():\n",
    "    global _word_id\n",
    "    _id = _word_id\n",
    "    _word_id += 1\n",
    "    return _id\n",
    "\n",
    "\n",
    "# 模拟10个待爬数据\n",
    "for url in links:\n",
    "    id_ = new_url_id()\n",
    "    link = Link(id=id_, url=url)\n",
    "    \n",
    "    print(link)\n",
    "    #  判重\n",
    "    print(  '- 未爬取？-->', bloom.is_visited(link.url) is False)\n",
    "    if bloom.is_visited(link.url) is False:    \n",
    "        # 模拟爬虫\n",
    "        link.get()\n",
    "        # 在爬取到的html里面提取跳转链接，然后塞到待爬去的列表中\n",
    "        _links = link.get_links()\n",
    "        links.extend(_links)\n",
    "        # 记录已爬的url\n",
    "        bloom.note_url(url)\n",
    "        # 记录网站id和url对应关系\n",
    "        doc_ids[id_] = url\n",
    "        # 记录body\n",
    "        doc_raw += str(id_) + '\\t' + link.body + '\\n\\n\\n'\n",
    "\n",
    "        # 获取过滤后的词列表\n",
    "        words = link.filter_words()\n",
    "        print('- 过滤、分词 -->', words)\n",
    "        # 创建索引\n",
    "        for word in words:\n",
    "            if word_ids.get(word, False) == False:\n",
    "                # 记录word对应的id\n",
    "                word_ids[word] = new_word_id()\n",
    "            word_id = word_ids[word]\n",
    "            # 创建索引\n",
    "            if word_id_position.get(word_id, None) != None: \n",
    "                row = word_id_position[word_id]\n",
    "                _inverted_index = inverted_index[row]\n",
    "                index = Index(word_id=word_id)\n",
    "                index.load(_inverted_index)\n",
    "                index.urls.append(link.url)\n",
    "                inverted_index[row] = index.to_inverted_index()\n",
    "            else:\n",
    "                row = len(inverted_index)\n",
    "                index = Index(word_id=word_id)\n",
    "                index.urls.append(link.url)\n",
    "                inverted_index.append(index.to_inverted_index())\n",
    "                word_id_position[word_id] = row\n",
    "    print('-'*20)\n",
    "\n",
    "\n",
    "# 查询\n",
    "\n",
    "test_word = 'test1'\n",
    "# 根据某个词获取词id\n",
    "test_word_id = word_ids[test_word]\n",
    "# 根据词id获取词id在倒排索引中的位置\n",
    "test_row = word_id_position[test_word_id]\n",
    "# 根据词id在倒排索引中的位置快速获取到词对应的网页编号列表\n",
    "_, test_urls = inverted_index[test_row].split('\\t')\n",
    "test_urls = test_urls.split(',')\n",
    "print(test_urls)\n",
    "# 根据网页编号列表获取url列表，返回给用户"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考资料\n",
    "\n",
    "- [极客时间-数据结构与算法之美-剖析搜索引擎背后的经典数据结构和算法](https://time.geekbang.org/column/article/79433)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
